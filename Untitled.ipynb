{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CheXpert inference with TorchXRayVision DenseNet-121\n",
        "\n",
        "\u0391\u03c5\u03c4\u03cc \u03c4\u03bf notebook \u03c1\u03c5\u03b8\u03bc\u03af\u03b6\u03b5\u03b9 \u03c4\u03bf\u03bd \u03c6\u03bf\u03c1\u03c4\u03c9\u03c4\u03ae \u03c4\u03bf\u03c5 dataset, \u03c6\u03bf\u03c1\u03c4\u03ce\u03bd\u03b5\u03b9 \u03c4\u03bf \u03c0\u03c1\u03bf\u03b5\u03ba\u03c0\u03b1\u03b9\u03b4\u03b5\u03c5\u03bc\u03ad\u03bd\u03bf \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03bf `torchxrayvision` \u03ba\u03b1\u03b9 \u03b4\u03af\u03bd\u03b5\u03b9 \u03c0\u03b1\u03c1\u03b1\u03b4\u03b5\u03af\u03b3\u03bc\u03b1\u03c4\u03b1 \u03b1\u03be\u03b9\u03bf\u03bb\u03cc\u03b3\u03b7\u03c3\u03b7\u03c2, inference \u03ba\u03b1\u03b9 Grad-CAM \u03b3\u03b9\u03b1 Chest X-ray \u03b5\u03b9\u03ba\u03cc\u03bd\u03b5\u03c2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from PIL import Image, ImageOps\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torchxrayvision as xrv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CheXpert dataset configuration\n",
        "data_root = Path(\"/data/CheXpert-v1.0-small\")  # \u03c0\u03c1\u03bf\u03c3\u03b1\u03c1\u03bc\u03cc\u03c3\u03c4\u03b5 \u03c3\u03c4\u03b7 \u03b4\u03b9\u03b1\u03b4\u03c1\u03bf\u03bc\u03ae \u03c4\u03bf\u03c5 extract\n",
        "train_csv = data_root / \"train.csv\"\n",
        "valid_csv = data_root / \"valid.csv\"\n",
        "\n",
        "IMG_SIZE = 224\n",
        "WEIGHTS_NAME = \"densenet121-res224-chex\"\n",
        "CLASS_NAMES = [\n",
        "    \"Atelectasis\",\n",
        "    \"Cardiomegaly\",\n",
        "    \"Consolidation\",\n",
        "    \"Edema\",\n",
        "    \"Enlarged Cardiomediastinum\",\n",
        "    \"Fracture\",\n",
        "    \"Lung Lesion\",\n",
        "    \"Lung Opacity\",\n",
        "    \"No Finding\",\n",
        "    \"Pleural Effusion\",\n",
        "    \"Pleural Other\",\n",
        "    \"Pneumonia\",\n",
        "    \"Pneumothorax\",\n",
        "    \"Support Devices\",\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_xrv_transform(img_size: int = IMG_SIZE):\n",
        "    center_crop = xrv.datasets.XRayCenterCrop()\n",
        "    resizer = xrv.datasets.XRayResizer(img_size)\n",
        "\n",
        "    def _transform(image: Image.Image) -> torch.Tensor:\n",
        "        arr = np.array(image.convert(\"L\"))\n",
        "        arr = center_crop(arr)\n",
        "        arr = resizer(arr)\n",
        "        arr = arr.astype(np.float32) / 255.0\n",
        "        tensor = torch.from_numpy(arr).unsqueeze(0)\n",
        "        return tensor\n",
        "\n",
        "    return _transform\n",
        "\n",
        "class CheXpertDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None, uncertain_policy=\"zeros\"):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.transform = transform or build_xrv_transform()\n",
        "\n",
        "        if uncertain_policy == \"zeros\":\n",
        "            processed = self.df[CLASS_NAMES].fillna(0.0).replace(-1, 0.0)\n",
        "        elif uncertain_policy == \"ones\":\n",
        "            processed = self.df[CLASS_NAMES].fillna(1.0).replace(-1, 1.0)\n",
        "        elif uncertain_policy == \"ignore\":\n",
        "            keep = self.df.dropna(subset=CLASS_NAMES)\n",
        "            processed = keep[CLASS_NAMES]\n",
        "            self.df = keep.reset_index(drop=True)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown uncertain_policy: {uncertain_policy}\")\n",
        "\n",
        "        self.labels = processed.astype(np.float32).values\n",
        "        self.paths = self.df[\"Path\"].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        rel_path = self.paths[idx]\n",
        "        image_path = self.root_dir / rel_path\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        tensor = self.transform(image)\n",
        "        target = torch.from_numpy(self.labels[idx])\n",
        "        return tensor, target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# build datasets and loaders\n",
        "train_dataset = CheXpertDataset(train_csv, data_root)\n",
        "valid_dataset = CheXpertDataset(valid_csv, data_root)\n",
        "\n",
        "batch_size = 16\n",
        "num_workers = 4\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load pretrained DenseNet-121 from torchxrayvision\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = xrv.models.DenseNet(weights=WEIGHTS_NAME)\n",
        "model = model.to(device).eval()\n",
        "print(f'Loaded {WEIGHTS_NAME} on {device}.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# evaluate ROC-AUC on the validation split\n",
        "model.eval()\n",
        "y_true, y_score = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in tqdm(valid_loader, desc=\"Validation\"):\n",
        "        inputs = inputs.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "        logits = model(inputs)\n",
        "        probs = torch.sigmoid(logits)\n",
        "\n",
        "        y_true.append(targets.cpu().numpy())\n",
        "        y_score.append(probs.cpu().numpy())\n",
        "\n",
        "y_true = np.concatenate(y_true, axis=0)\n",
        "y_score = np.concatenate(y_score, axis=0)\n",
        "\n",
        "roc_aucs = {}\n",
        "for idx, name in enumerate(CLASS_NAMES):\n",
        "    try:\n",
        "        roc_aucs[name] = roc_auc_score(y_true[:, idx], y_score[:, idx])\n",
        "    except ValueError:\n",
        "        roc_aucs[name] = float('nan')\n",
        "\n",
        "roc_aucs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# single-image inference demo\n",
        "example_idx = 0\n",
        "example_rel_path = valid_dataset.paths[example_idx]\n",
        "example_image = Image.open(data_root / example_rel_path).convert('RGB')\n",
        "input_tensor = valid_dataset.transform(example_image).unsqueeze(0).to(device)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(input_tensor)\n",
        "    probs = torch.sigmoid(logits)[0].cpu().numpy()\n",
        "\n",
        "top = sorted(zip(CLASS_NAMES, probs), key=lambda item: item[1], reverse=True)\n",
        "print(f'Top findings for {example_rel_path}:')\n",
        "for name, p in top[:5]:\n",
        "    print(f'  {name:<26}: {p:.3f}')\n",
        "\n",
        "example_image.resize((IMG_SIZE, IMG_SIZE))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grad-CAM utilities (\u03af\u03b4\u03b9\u03b1 \u03bb\u03bf\u03b3\u03b9\u03ba\u03ae \u03bc\u03b5 \u03c4\u03bf backend)\n",
        "def _register_backward_hook(module, hook):\n",
        "    if hasattr(module, 'register_full_backward_hook'):\n",
        "        return module.register_full_backward_hook(hook)\n",
        "    return module.register_backward_hook(lambda m, grad_in, grad_out: hook(m, grad_in, grad_out))\n",
        "\n",
        "def compute_grad_cam(model, input_tensor, target_layer, target_index=None):\n",
        "    activations, gradients = [], []\n",
        "\n",
        "    def forward_hook(_module, _inp, output):\n",
        "        activations.append(output.detach())\n",
        "\n",
        "    def backward_hook(_module, _grad_input, grad_output):\n",
        "        gradients.append(grad_output[0].detach())\n",
        "\n",
        "    handle_fwd = target_layer.register_forward_hook(forward_hook)\n",
        "    handle_bwd = _register_backward_hook(target_layer, backward_hook)\n",
        "\n",
        "    input_tensor = input_tensor.requires_grad_(True)\n",
        "    logits = model(input_tensor)\n",
        "\n",
        "    if target_index is None:\n",
        "        probs = torch.sigmoid(logits)[0]\n",
        "        target_index = int(torch.argmax(probs).item())\n",
        "\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    score = logits[:, target_index]\n",
        "    score.backward()\n",
        "\n",
        "    handle_fwd.remove()\n",
        "    handle_bwd.remove()\n",
        "\n",
        "    if not activations or not gradients:\n",
        "        return logits.detach(), None\n",
        "\n",
        "    grad = gradients[0]\n",
        "    act = activations[0]\n",
        "\n",
        "    weights = grad.mean(dim=(2, 3), keepdim=True)\n",
        "    cam = torch.relu((weights * act).sum(dim=1, keepdim=True))\n",
        "    cam = torch.nn.functional.interpolate(cam, size=(IMG_SIZE, IMG_SIZE), mode='bilinear', align_corners=False)\n",
        "    cam = cam.squeeze().cpu()\n",
        "    cam = cam - cam.min()\n",
        "    cam = cam / (cam.max() + 1e-8)\n",
        "    return logits.detach(), cam.numpy()\n",
        "\n",
        "def build_heatmap_overlay(base_image: Image.Image, heatmap, alpha: float = 0.45):\n",
        "    resized = base_image.convert('RGB').resize((IMG_SIZE, IMG_SIZE))\n",
        "    heat = Image.fromarray(np.uint8(heatmap * 255), mode='L')\n",
        "    heat = heat.resize(resized.size, resample=Image.BILINEAR)\n",
        "    colorized = ImageOps.colorize(heat, black='#0b1f3a', white='#f97316')\n",
        "    return Image.blend(resized, colorized, alpha)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grad-CAM visualization\n",
        "model.eval()\n",
        "target_layer = model.features.denseblock4\n",
        "logits, heatmap = compute_grad_cam(model, input_tensor, target_layer)\n",
        "\n",
        "if heatmap is not None:\n",
        "    overlay = build_heatmap_overlay(example_image, heatmap)\n",
        "    display(overlay)\n",
        "else:\n",
        "    print('Grad-CAM heatmap could not be computed.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \u0395\u03bd\u03b7\u03bc\u03ad\u03c1\u03c9\u03c3\u03b7 metadata \u03b3\u03b9\u03b1 \u03c4\u03bf FastAPI app\n",
        "metadata = {\n",
        "    'dataset': 'chexpert',\n",
        "    'task': 'multi-label-binary',\n",
        "    'img_size': IMG_SIZE,\n",
        "    'n_channels': 1,\n",
        "    'mean': [0.5],\n",
        "    'std': [0.5],\n",
        "    'pretrained_source': 'torchxrayvision',\n",
        "    'weights': WEIGHTS_NAME,\n",
        "    'class_names': CLASS_NAMES,\n",
        "}\n",
        "\n",
        "metadata_path = Path('medmnist_web/models/metadata.json')\n",
        "metadata_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \u0395\u03be\u03b1\u03b3\u03c9\u03b3\u03ae \u03b5\u03bd\u03b4\u03b5\u03b9\u03ba\u03c4\u03b9\u03ba\u03ce\u03bd \u03b5\u03b9\u03ba\u03cc\u03bd\u03c9\u03bd \u03b3\u03b9\u03b1 \u03c4\u03bf web UI\n",
        "sample_dir = Path('sample_uploads')\n",
        "sample_dir.mkdir(exist_ok=True)\n",
        "\n",
        "raw_valid = CheXpertDataset(valid_csv, data_root, transform=lambda x: x)\n",
        "for idx in range(5):\n",
        "    image, target = raw_valid[idx]\n",
        "    labels = [CLASS_NAMES[i].replace(' ', '_') for i, value in enumerate(target.numpy()) if value > 0.5]\n",
        "    suffix = '_'.join(labels) if labels else 'No_Finding'\n",
        "    filename = f'valid_{idx}_{suffix}.png'\n",
        "    image.save(sample_dir / filename)\n",
        "\n",
        "print('\u2705 Saved samples in sample_uploads/')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}